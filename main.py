import json
import re

import pandas as pd
import pymorphy2
import vk_api
import xlsxwriter
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from num2words import num2words
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import NuSVC
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sn
from tqdm import tqdm

from LikesDivider import LikesDivider

rt = RegexpTokenizer(r'\w+')
morph = pymorphy2.MorphAnalyzer()
tf_idf = TfidfVectorizer()
nu_svc = NuSVC(verbose=True, kernel='linear', max_iter=-1)
hashed_stop_words = [hash(word) for word in stopwords.words()]

# -*- coding: utf-8 -*-


def preprocess_text(tokens: str) -> str:
    tokens = rt.tokenize(re.sub('http[s]?://\S+', '', tokens))
    text = ""

    for i, w in enumerate(tokens):
        tokens[i] = morph.parse(num2words(w) if w.isnumeric() else w)[0].normal_form

    tokens = [words for words in tokens if not hash(words) in hashed_stop_words]

    # i in range(1, len(tokens) - 1):
    #    text += tokens[i - 1] + tokens[i] + tokens[i + 1] + '\t'

    for w in tokens:
        text += w + " "

    return text


def preprocess_texts(texts: [str]) -> [str]:
    processed_texts = []
    for text in tqdm(texts, desc="Preprocessing"):
        processed_texts.append(preprocess_text(text))

    return processed_texts


def tf_idf_matrix_to_exel(terms, tf_idf_matrix):
    workbook = xlsxwriter.Workbook('matrix.xlsx')
    worksheet = workbook.add_worksheet('sheet')
    row = 0
    col = 0
    for word in tqdm(terms, desc='Writing to exel'):
        worksheet.write(row, col, word)
        col += 1
    for post in tf_idf_matrix:
        row += 1
        col = 0
        for word in post:
            worksheet.write(row, col, word)
            col += 1
    workbook.close()


def auth_handler():
    return input("Enter authentication code: "), True


def get_texts_and_likes_from_json(json_data):
    texts = []
    likes = []
    for item in json_data['items']:
        likes.append(item['likes']['count'])
        texts.append(item['text'])

    return texts, likes


def get_group_posts_and_likes(login, password, group_domain) -> (str, str):
    vk_session = vk_api.VkApi(
        login, password,
        auth_handler=auth_handler
    )

    vk_session.auth()

    vk = vk_session.get_api()

    json_data = vk.wall.get(domain=group_domain)
    print("Response from vk:\n", json_data)

    return get_texts_and_likes_from_json(json_data)


def get_test_data_from_file(filename) -> (str, str):
    """
    Used for demonstration on test json
    :return:
    """
    with open(filename, "r", encoding='utf-8') as json_file:
        json_load = json.load(json_file)
    return get_texts_and_likes_from_json(json_load)


def split_data(x: [], y: [], test_size=1):
    if len(x) != len(y):
        raise AttributeError("x and y sizes does not match")
    if len(x) <= test_size:
        raise AttributeError("Too small test size")

    return x[test_size:], x[:test_size], y[test_size:], y[:test_size]


def get_text():
    return 'ÐŸÑ€Ð¸Ð¼Ð¸ ÑƒÑ‡Ð°ÑÑ‚Ð¸Ðµ Ð² Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð½Ð¾Ð¼ Ð¾Ð½Ð»Ð°Ð¹Ð½-Ñ…Ð°ÐºÐ°Ñ‚Ð¾Ð½Ðµ Â«Moscow City Hack 2022Â» Ð¾Ñ‚ ÐÐ³ÐµÐ½Ñ‚ÑÑ‚Ð²Ð° Ð¸Ð½Ð½Ð¾Ð²Ð°Ñ†Ð¸Ð¹ ÐœÐ¾ÑÐºÐ²Ñ‹!Ð¢ÐµÐ±Ñ ' \
           'Ð¶Ð´ÑƒÑ‚:ðŸ† ÐŸÑ€Ð¸Ð·Ð¾Ð²Ð¾Ð¹ Ñ„Ð¾Ð½Ð´ 3 400 000 â‚½;ðŸ’» Ð—Ð°Ð´Ð°Ñ‡Ð¸ Ð¿Ð¾ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð² Ð´Ð»Ñ Ð¼Ð¾Ñ‚Ð¸Ð²Ð°Ñ†Ð¸Ð¸ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ð¾Ð², Ð¿Ñ€Ð¸Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ ' \
           'Ð²Ð¾Ð»Ð¾Ð½Ñ‚ÐµÑ€Ð¾Ð², Ñ†Ð¸Ñ„Ñ€Ð¾Ð²Ð¾Ð³Ð¾ Ð¼Ð°Ñ€ÐºÐµÑ‚Ð¸Ð½Ð³Ð°, Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¾Ð·Ð°Ð¼ÐµÑ‰ÐµÐ½Ð¸Ñ Ð¸ Ñ€Ð°Ð·Ð¾Ð±Ð»Ð°Ñ‡ÐµÐ½Ð¸Ñ fake news;ðŸ§ Ð­ÐºÑÐ¿ÐµÑ€Ñ‚Ñ‹ Ð¾Ñ‚ ÐºÑ€ÑƒÐ¿Ð½Ð¾Ð³Ð¾ ' \
           'Ð±Ð¸Ð·Ð½ÐµÑÐ° Ð¸ ÐŸÑ€Ð°Ð²Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð° ÐœÐ¾ÑÐºÐ²Ñ‹;ðŸŽ“ ÐžÐ±Ñ€Ð°Ð·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð° Ñ Ð¼Ð°ÑÑ‚ÐµÑ€-ÐºÐ»Ð°ÑÑÐ°Ð¼Ð¸ Ð¸ Ð¸Ð½Ñ‚ÐµÐ½ÑÐ¸Ð²Ð°Ð¼Ð¸;ðŸŽ ÐšÑ€Ð°ÑÐ¾Ñ‡Ð½Ñ‹Ð¹ ' \
           'Ð¼ÐµÑ€Ñ‡, Ð¿Ð¾Ð´Ð°Ñ€ÐºÐ¸ Ð¾Ñ‚ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð¾Ð² Ð¸ Ð¼Ð½Ð¾Ð³Ð¾ ÐºÑ€ÑƒÑ‚Ñ‹Ñ… Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÐµÐ¹ ðŸ”¥ðŸ“… Ð¥Ð°ÐºÐ°Ñ‚Ð¾Ð½ Ð¿Ñ€Ð¾Ð¹Ð´ÐµÑ‚ 10-13 Ð¸ÑŽÐ½Ñ 2022 ' \
           'Ð³Ð¾Ð´Ð°Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐ¹ÑÑ ÑƒÐ¶Ðµ ÑÐµÐ¹Ñ‡Ð°Ñ!https://bit.ly/3lhAjepÐ£Ð·Ð½Ð°Ñ‚ÑŒ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð¸ Ð½Ð°Ð¹Ñ‚Ð¸ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñƒ Ð¼Ð¾Ð¶Ð½Ð¾ Ð² Ð½Ð°ÑˆÐµÐ¼ ' \
           'Telegram-Ñ‡Ð°Ñ‚Ðµt.me/MoscowCityHack '


def data_from_skillbox_csv():
    df = pd.read_csv('data/vk_skillbox.csv')

    return df['text'].apply(str).tolist(), df['likes'].tolist()


def check_quality(texts, likes):
    texts = preprocess_texts(texts)
    likes_divider = LikesDivider(likes, 3)
    likes_groups = [likes_divider.get_like_group(like) for like in likes]
    print(likes_groups)

    test_size = 100
    train_texts, test_texts, train_likes_groups, test_likes_groups = train_test_split(texts, likes_groups,
                                                                                      test_size=test_size,
                                                                                      shuffle=False)

    tf_idf_matrix = tf_idf.fit_transform(test_texts + train_texts).toarray()
    terms = tf_idf.get_feature_names_out()
    tf_idf_matrix_to_exel(terms, tf_idf_matrix)
    nu_svc.fit(tf_idf_matrix[test_size:], train_likes_groups)

    predicted_likes_groups = nu_svc.predict(tf_idf_matrix[:test_size])

    cm = confusion_matrix(test_likes_groups, predicted_likes_groups)

    df_cm = pd.DataFrame(cm, likes_divider.get_likes_groups(), likes_divider.get_likes_groups())
    sn.heatmap(df_cm, annot=True)
    plt.show()


def main():
    texts, likes = data_from_skillbox_csv()
    # texts, likes = get_test_data_from_file('data/data_from_javatutorial.json')
    check_quality(texts, likes)


if __name__ == '__main__':
    main()
